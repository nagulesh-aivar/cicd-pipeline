{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install boto3 pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFL9NWaGxdPL",
        "outputId": "3c7ab124-7482-4252-8ebd-9261cda25732"
      },
      "outputs": [],
      "source": [
        "\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJitr6IL03XK"
      },
      "outputs": [],
      "source": [
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAVSIIP37VERPGV2KV\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"ap-south-1\"\n",
        "\n",
        "s3 = boto3.client(\"s3\")\n",
        "bucket_name = \"ginth-etl\"\n",
        "prefix = \"supply_note/item_wise_grn_data/source/\"\n",
        "processed = \"supply_note/item_wise_grn_data/processed/\"\n",
        "seeding_grn_path = \"supply_note/item_wise_grn_data/seeding_data/grn\"\n",
        "seeding_invoice_path = \"supply_note/item_wise_grn_data/seeding_data/invoice\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-bVnDpz027L",
        "outputId": "8f5893c9-9de0-426c-9239-a1139d65de9b"
      },
      "outputs": [],
      "source": [
        "objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "csv_keys = [obj['Key'] for obj in objects.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
        "csv_keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wipj04VG0_Go",
        "outputId": "381d04df-f155-4e74-c3cc-d93f82938919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined shape: (522, 59)\n"
          ]
        }
      ],
      "source": [
        "all_data = []\n",
        "\n",
        "for key in csv_keys:\n",
        "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
        "    csv_str = obj['Body'].read().decode('utf-8')\n",
        "    if \"NO DATA FOUND FOR THIS QUERY\" in csv_str:\n",
        "        continue  # skip invalid files\n",
        "\n",
        "    df = pd.read_csv(StringIO(csv_str))\n",
        "    all_data.append(df)\n",
        "\n",
        "combined_df = pd.concat(all_data, ignore_index=True)\n",
        "print(\"Combined shape:\", combined_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill all null values with 0 BEFORE deduplication\n",
        "combined_df = combined_df.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FDOeLpBG1E1Y"
      },
      "outputs": [],
      "source": [
        "# Rank by data_version (higher = latest)\n",
        "combined_df['rank'] = combined_df.groupby(\n",
        "    ['Item Name', 'PO No.', 'GRN No.', 'Seller Invoice No']\n",
        ")['data_version'].rank(method='first', ascending=False)\n",
        "\n",
        "# Keep only top-ranked rows\n",
        "clean_df = combined_df[combined_df['rank'] == 1].drop(columns=['rank'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6CFua161IZL",
        "outputId": "369c1282-3319-40f3-f2cd-661d903d2207"
      },
      "outputs": [],
      "source": [
        "clean_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6JncNL_1b_E"
      },
      "outputs": [],
      "source": [
        "clean_df = clean_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gUl1jRkE1kdm"
      },
      "outputs": [],
      "source": [
        "# Convert ALL numeric columns to proper numeric types\n",
        "numeric_columns = [\n",
        "    'Price', 'Received Qty', 'Returned Qty', 'Discount', 'Tax',\n",
        "    'SGST Tax', 'SGST Tax Amount', 'CGST Tax', 'CGST Tax Amount',\n",
        "    'IGST Tax', 'IGST Tax Amount', 'cess', 'SubTotal',\n",
        "    'VAT(Amount)', 'Item TCS(Amount)', 'Tax Amount', 'Bill TCS',\n",
        "    'Delivery Charges', 'Delivery Charges Tax(%)', 'Additional Charges',\n",
        "    'INV Discount', 'RoundOff', 'Total'\n",
        "]\n",
        "\n",
        "for col in numeric_columns:\n",
        "    if col in clean_df.columns:\n",
        "        clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_2PGRed4uOt"
      },
      "outputs": [],
      "source": [
        "#clean_df.to_csv(\"cleaned_item_wise_grn.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIRECTLY USE clean_df - no need to save/read CSV\n",
        "df = clean_df  # Just assign it directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NafR2fv5WDJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON file saved: itemwise_grn_output_20251113_143304.json\n"
          ]
        }
      ],
      "source": [
        "# Load your CSV\n",
        "#df = pd.read_csv(\"/content/cleaned_item_wise_grn.csv\")\n",
        "\n",
        "# Clean up column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Replace NaN with None for JSON\n",
        "df = df.where(pd.notnull(df), None)\n",
        "\n",
        "# Build GRN-wise grouped structure\n",
        "final_data = []\n",
        "\n",
        "grouped = df.groupby(['GRN No.', 'PO No.'])\n",
        "\n",
        "for (grn_no, po_no), group in grouped:\n",
        "    first_row = group.iloc[0]\n",
        "\n",
        "    record = {\n",
        "        \"grn_number\": str(grn_no) if grn_no is not None else None,  # Ensure string\n",
        "        \"purchase_order_id\": str(po_no) if po_no is not None else None,  # Ensure string\n",
        "        \"vendor_id\": str(first_row.get(\"Supplier\")) if first_row.get(\"Supplier\") is not None else None,  # Ensure string\n",
        "        \"invoice_id\": str(first_row.get(\"Seller Invoice No\")) if first_row.get(\"Seller Invoice No\") is not None else None,  # Ensure string\n",
        "        \"received_date\": first_row.get(\"GRN Created At\"),  # Keep as is (date string)\n",
        "        \"received_by\": str(first_row.get(\"Created By\")) if first_row.get(\"Created By\") is not None else None,  # Ensure string\n",
        "        \"item_list\": []\n",
        "    }\n",
        "\n",
        "    # Loop over each item in that GRN\n",
        "    for idx, row in group.iterrows():\n",
        "        item = {\n",
        "            \"item_sequence\": int(row.get(\"S.No.\")) if pd.notna(row.get(\"S.No.\")) else None,  # Number\n",
        "            \"item_description\": str(row.get(\"Item Name\")) if row.get(\"Item Name\") is not None else None,  # String\n",
        "            \"hsn_code\": str(row.get(\"HSN No.\")) if row.get(\"HSN No.\") is not None else None,  # String\n",
        "            \"quantity\": float(row.get(\"Received Qty\")) if pd.notna(row.get(\"Received Qty\")) else 0,  # Number\n",
        "            \"unit_of_measurement\": str(row.get(\"Unit\")) if row.get(\"Unit\") is not None else None,  # String\n",
        "            \"rate\": float(row.get(\"Price\")) if pd.notna(row.get(\"Price\")) else 0,  # Number\n",
        "            \"cgst_rate\": float(row.get(\"CGST Tax\")) if pd.notna(row.get(\"CGST Tax\")) else 0,  # Number\n",
        "            \"cgst_amount\": float(row.get(\"CGST Tax Amount\")) if pd.notna(row.get(\"CGST Tax Amount\")) else 0,  # Number\n",
        "            \"sgst_rate\": float(row.get(\"SGST Tax\")) if pd.notna(row.get(\"SGST Tax\")) else 0,  # Number\n",
        "            \"sgst_amount\": float(row.get(\"SGST Tax Amount\")) if pd.notna(row.get(\"SGST Tax Amount\")) else 0,  # Number\n",
        "            \"igst_rate\": float(row.get(\"IGST Tax\")) if pd.notna(row.get(\"IGST Tax\")) else 0,  # Number\n",
        "            \"igst_amount\": float(row.get(\"IGST Tax Amount\")) if pd.notna(row.get(\"IGST Tax Amount\")) else 0,  # Number\n",
        "            \"cess_rate\": 0,  # Number\n",
        "            \"cess_amount\": float(row.get(\"cess\")) if pd.notna(row.get(\"cess\")) else 0,  # Number\n",
        "            \"item_total_before_tax\": float(row.get(\"SubTotal\")) if pd.notna(row.get(\"SubTotal\")) else 0,  # Number\n",
        "            \"total_tax_amount\": float(row.get(\"Tax Amount\")) if pd.notna(row.get(\"Tax Amount\")) else 0,  # Number\n",
        "            \"item_total_amount\": float(row.get(\"Total\")) if pd.notna(row.get(\"Total\")) else 0  # Number\n",
        "        }\n",
        "        record[\"item_list\"].append(item)\n",
        "\n",
        "    # Add GRN-level totals - these will be numbers (float)\n",
        "    record[\"total_amount_without_tax\"] = float(group[\"SubTotal\"].sum()) if \"SubTotal\" in group.columns else 0.0\n",
        "    record[\"total_amount\"] = float(group[\"Total\"].sum()) if \"Total\" in group.columns else 0.0\n",
        "    record[\"total_gst\"] = float(group[\"Tax Amount\"].sum()) if \"Tax Amount\" in group.columns else 0.0\n",
        "    record[\"total_cgst\"] = float(group[\"CGST Tax Amount\"].sum()) if \"CGST Tax Amount\" in group.columns else 0.0\n",
        "    record[\"total_sgst\"] = float(group[\"SGST Tax Amount\"].sum()) if \"SGST Tax Amount\" in group.columns else 0.0\n",
        "    record[\"total_igst\"] = float(group[\"IGST Tax Amount\"].sum()) if \"IGST Tax Amount\" in group.columns else 0.0\n",
        "    record[\"freight_charges\"] = float(group[\"Delivery Charges\"].sum()) if \"Delivery Charges\" in group.columns else 0.0\n",
        "    record[\"notes\"] = str(first_row.get(\"Remarks\")) if first_row.get(\"Remarks\") is not None else None  # String\n",
        "    record[\"spoc_email\"] = \"\"  # String (not in CSV)\n",
        "    record[\"spoc_phone\"] = \"\"  # String (not in CSV)\n",
        "\n",
        "\n",
        "    final_data.append(record)\n",
        "\n",
        "# Build final JSON\n",
        "final_json = {\"data\": final_data}\n",
        "\n",
        "# Generate timestamp\n",
        "current_datetime = datetime.now()\n",
        "timestamp = current_datetime.strftime(\"%Y%m%d_%H%M%S\")  # Format: 20251111_143025\n",
        "\n",
        "# Create filename with timestamp\n",
        "grn_output_filename = f\"itemwise_grn_output_{timestamp}.json\"\n",
        "\n",
        "# Save output JSON with timestamp\n",
        "with open(grn_output_filename, \"w\") as f:\n",
        "    json.dump(final_json, f, indent=2)\n",
        "\n",
        "print(f\"JSON file saved: {grn_output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Upload to S3\n",
        "# -----------------------------\n",
        "\n",
        "grn_output_path = f\"{seeding_grn_path}/{grn_output_filename}\"  # Path in S3\n",
        "\n",
        "# Upload file\n",
        "s3.upload_file(grn_output_filename, bucket_name, grn_output_path)\n",
        "\n",
        "print(f\"GRN File uploaded to {grn_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzb05a2q_trC"
      },
      "outputs": [],
      "source": [
        "# Assuming you have clean_df after deduplication\n",
        "\n",
        "# Step 1: Create a list to collect all attachments\n",
        "invoice_data = []\n",
        "\n",
        "for idx, row in clean_df.iterrows():\n",
        "    po_no = row.get('PO No.')\n",
        "    grn_no = row.get('GRN No.')\n",
        "    \n",
        "    # Collect all non-empty attachments\n",
        "    attachments = [\n",
        "        row.get('Attachment-1'),\n",
        "        row.get('Attachment-2'),\n",
        "        row.get('Attachment-3'),\n",
        "        row.get('Attachment-4'),\n",
        "        row.get('Attachment-5')\n",
        "    ]\n",
        "    \n",
        "    # Add each attachment as a separate row\n",
        "    for attachment in attachments:\n",
        "        if attachment and attachment != '-' and str(attachment).strip():\n",
        "            invoice_data.append({\n",
        "                'PO': po_no,\n",
        "                'GRN': grn_no,\n",
        "                'Attachment': attachment\n",
        "            })\n",
        "\n",
        "# Step 2: Create DataFrame\n",
        "invoice_df = pd.DataFrame(invoice_data)\n",
        "\n",
        "# Step 3: Remove duplicates based on all three columns\n",
        "invoice_df = invoice_df.drop_duplicates(subset=['PO', 'GRN', 'Attachment'])\n",
        "\n",
        "# Step 4: Reset index\n",
        "invoice_df = invoice_df.reset_index(drop=True)\n",
        "\n",
        "# Step 5: Save with timestamp\n",
        "current_datetime = datetime.now()\n",
        "timestamp = current_datetime.strftime(\"%Y%m%d_%H%M%S\")\n",
        "invoice_output_filename = f\"invoice_attachments_{timestamp}.csv\"\n",
        "\n",
        "invoice_df.to_csv(invoice_output_filename, index=False)\n",
        "\n",
        "print(f\"‚úÖ Invoice CSV saved: {invoice_output_filename}\")\n",
        "print(f\"Total records: {len(invoice_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Upload to S3\n",
        "# -----------------------------\n",
        "\n",
        "invoice_output_path = f\"{seeding_invoice_path}/{invoice_output_filename}\"  # Path in S3\n",
        "\n",
        "# Upload file\n",
        "s3.upload_file(invoice_output_filename, bucket_name, invoice_output_path)\n",
        "\n",
        "print(f\"Invoice File uploaded to {invoice_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sqs = boto3.client(\n",
        "    \"sqs\",\n",
        "    aws_access_key_id=\"AKIAVSIIP37VBKM4JMMM\",\n",
        "    aws_secret_access_key=\"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\",\n",
        "    region_name=\"ap-south-1\",\n",
        ")\n",
        "\n",
        "queue_url = \"https://sqs.ap-south-1.amazonaws.com/382806777834/invoice-processing-queue\"\n",
        "\n",
        "message = {\n",
        "    \"file_path\": invoice_output_path,\n",
        "    \"file_name\": invoice_output_filename,\n",
        "    \"bucket_name\": bucket_name,\n",
        "    \"folder_name\": seeding_invoice_path,\n",
        "}\n",
        "\n",
        "sqs.send_message(QueueUrl=queue_url, MessageBody=json.dumps(message))\n",
        "\n",
        "print(\"Invoice File Pushed into SQS Queue successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# API Config (replace with your values)\n",
        "API_BASE_URL = \"http://127.0.0.1:8005\"  # Local FastAPI URL (or ngrok/public URL if remote)\n",
        "ENDPOINT = f\"{API_BASE_URL}/api/v1/documents/create\"\n",
        "CLIENT_ID = \"1015aca0-646c-4815-9f8c-44c4843d35e2\"  # e.g., \"123e4567-e89b-12d3-a456-426614174000\" (from your DB)\n",
        "COLLECTION_NAME = \"grn\"  # Your MongoDB collection name\n",
        "CREATED_BY = \"user ID\"  # e.g., \"987fcdeb-51a2-34d5-c678-9abcdef012345\" (user ID)\n",
        "\n",
        "\n",
        "# Payload (uses final_json from previous cell)\n",
        "payload = {\n",
        "    \"client_id\": CLIENT_ID,\n",
        "    \"collection_name\": COLLECTION_NAME,\n",
        "    \"data\": final_data,  # Your generated {\"data\": [...]} \n",
        "    \"created_by\": CREATED_BY\n",
        "}\n",
        "\n",
        "# Headers for auth\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "# Send POST\n",
        "response = requests.post(ENDPOINT, json=payload, headers=headers)\n",
        "\n",
        "# Results\n",
        "if response.status_code == 201:\n",
        "    print(\"‚úÖ Success! GRN documents created in MongoDB.\")\n",
        "    print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error {response.status_code}: {response.text}\")\n",
        "\n",
        "# Save response\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "with open(f\"api_response_{timestamp}.json\", \"w\") as f:\n",
        "    json.dump(response.json(), f, indent=2)\n",
        "print(f\"Response saved: api_response_{timestamp}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move processed files to 'processed' folder\n",
        "moved_count = 0\n",
        "\n",
        "for source_key in csv_keys:\n",
        "    try:\n",
        "        filename = source_key.split(\"/\")[-1]\n",
        "        destination_key = f\"{processed}{filename}\"\n",
        "\n",
        "        # Copy to processed folder\n",
        "        s3.copy_object(\n",
        "            CopySource={\"Bucket\": bucket_name, \"Key\": source_key},\n",
        "            Bucket=bucket_name,\n",
        "            Key=destination_key,\n",
        "        )\n",
        "\n",
        "        # Delete from source\n",
        "        s3.delete_object(Bucket=bucket_name, Key=source_key)\n",
        "\n",
        "        moved_count += 1\n",
        "        print(f\"‚úÖ {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to move {filename}: {str(e)}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"‚úÖ Successfully moved {moved_count}/{len(csv_keys)} files\")\n",
        "print(f\"üìÅ Location: s3://{bucket_name}/{processed}\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
